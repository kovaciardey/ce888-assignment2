{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alex_net_flame_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNSiovD2xyRrJDSZ4ExcYSL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kovaciardey/ce888-assignment/blob/main/alex_net_flame_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPokfjdaVB1p"
      },
      "source": [
        "# ALexNet CNN training with the FLAME dataset \r\n",
        "\r\n",
        "This implementation of the AlexNet CNN has been created following a tutorial on [this](https://towardsdatascience.com/implementing-alexnet-cnn-architecture-using-tensorflow-2-0-and-keras-2113e090ad98) article on the TowardsDataScience website. Some tweaks have been made to the functionality of the workflow to suit the FLAME dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4A_YiRwnWtr"
      },
      "source": [
        "# importing the libraries\r\n",
        "\r\n",
        "from numpy import load\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tensorflow import keras\r\n",
        "import os\r\n",
        "import time\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY9Yio6-WKPG"
      },
      "source": [
        "## !! IMPORTANT !!\r\n",
        "\r\n",
        "It is recomended to have the data files generated by the data_splitting notebook at the same level in folder as this notebook.\r\n",
        "\r\n",
        "Additionally, it should be mentioned that the code presented in this notebook had been executed locally from the PC and not on Google colab. It is my recommendation to do the same as there may be issues with loading the data. \r\n",
        "\r\n",
        "The python script has been converted into this notebook to enable the execution of the code in snippets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fjE13I8L_NB"
      },
      "source": [
        "# loading the data and verifying the correct shape and size\r\n",
        "\r\n",
        "set_size = 500\r\n",
        "\r\n",
        "# the name of the 4 files to load. The path can be updated here to load the data from a different place.\r\n",
        "train_images = load(\"training_flame_photos.npy\")\r\n",
        "train_labels = load(\"training_flame_labels.npy\")\r\n",
        "\r\n",
        "test_images = load(\"test_flame_photos.npy\")\r\n",
        "test_labels = load(\"test_flame_labels.npy\")\r\n",
        "\r\n",
        "CLASSES = [\"No Fire\", \"Fire\"]\r\n",
        "\r\n",
        "validation_images, validation_labels = train_images[:set_size], train_labels[:set_size]\r\n",
        "train_images, train_labels = train_images[set_size:], train_labels[set_size:]\r\n",
        "\r\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\r\n",
        "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\r\n",
        "\r\n",
        "print(\"  Training Data Shape:\", train_ds)\r\n",
        "print(\"      Test Data Shape:\", test_ds)\r\n",
        "print(\"Validation Data Shape:\", validation_ds)\r\n",
        "\r\n",
        "train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()\r\n",
        "test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()\r\n",
        "validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()\r\n",
        "\r\n",
        "print(\"  Training data size:\", train_ds_size)\r\n",
        "print(\"      Test data size:\", test_ds_size)\r\n",
        "print(\"Validation data size:\", validation_ds_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGxjIUZFqTe2"
      },
      "source": [
        "# processing the images into the correct size\r\n",
        "\r\n",
        "def process_images(image, label):\r\n",
        "    # Normalize images to have a mean of 0 and standard deviation of 1\r\n",
        "    image = tf.image.per_image_standardization(image)\r\n",
        "    # Resize images from 32x32 to 277x277\r\n",
        "    image = tf.image.resize(image, (227, 227))\r\n",
        "    return image, label\r\n",
        "\r\n",
        "train_ds = (train_ds\r\n",
        "                  .map(process_images)\r\n",
        "                  .shuffle(buffer_size=train_ds_size)\r\n",
        "                  .batch(batch_size=32, drop_remainder=True))\r\n",
        "test_ds = (test_ds\r\n",
        "                  .map(process_images)\r\n",
        "                  .shuffle(buffer_size=train_ds_size)\r\n",
        "                  .batch(batch_size=32, drop_remainder=True))\r\n",
        "validation_ds = (validation_ds\r\n",
        "                  .map(process_images)\r\n",
        "                  .shuffle(buffer_size=train_ds_size)\r\n",
        "                  .batch(batch_size=32, drop_remainder=True))\r\n",
        "\r\n",
        "print(train_ds)\r\n",
        "print(test_ds)\r\n",
        "print(validation_ds)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCRYMVPhqXa_"
      },
      "source": [
        "# configuring the model\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
        "    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\r\n",
        "    keras.layers.Flatten(),\r\n",
        "    keras.layers.Dense(4096, activation='relu'),\r\n",
        "    keras.layers.Dropout(0.5),\r\n",
        "    keras.layers.Dense(4096, activation='relu'),\r\n",
        "    keras.layers.Dropout(0.5),\r\n",
        "    keras.layers.Dense(10, activation='softmax')\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy'])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQl4teIQWwSD"
      },
      "source": [
        "# training the model\r\n",
        "model.fit(train_ds,\r\n",
        "          epochs=50,\r\n",
        "          validation_data=validation_ds,\r\n",
        "          validation_freq=1,\r\n",
        "          callbacks=[tensorboard_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQNgaS0i3AAW"
      },
      "source": [
        "# resizing the images in the original \r\n",
        "\r\n",
        "print(test_ds_size)\r\n",
        "\r\n",
        "def process_images2(image):\r\n",
        "    image = tf.image.per_image_standardization(image)\r\n",
        "    image = tf.image.resize(image, (227, 227))\r\n",
        "    return image\r\n",
        "\r\n",
        "new_train_ds = tf.data.Dataset.from_tensor_slices((test_images, ))\r\n",
        "\r\n",
        "new_train_ds = (new_train_ds.map(process_images2).batch(batch_size=32, drop_remainder=True))\r\n",
        "\r\n",
        "print(new_train_ds)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyy-cswxqbAh"
      },
      "source": [
        "y_pred = np.argmax(model.predict(new_train_ds), axis=-1)\r\n",
        "\r\n",
        "test_labels = test_labels.reshape((test_labels.size,))\r\n",
        "\r\n",
        "print(y_pred, y_pred.size)\r\n",
        "print(test_labels, test_labels.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFNwtbw22qnK"
      },
      "source": [
        "# hacky way of updating the size of the predictions set after the bug introduced by the batching.\r\n",
        "\r\n",
        "predictions = np.zeros((test_labels.size,), dtype=int)\r\n",
        "\r\n",
        "for i in range(y_pred.size):\r\n",
        "  predictions[i] = y_pred[i]\r\n",
        "\r\n",
        "print(y_pred, y_pred.size)\r\n",
        "print(predictions, predictions.size)\r\n",
        "print(test_labels, test_labels.size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh3-9VvB2YRu"
      },
      "source": [
        "# generating the confusion matrix \r\n",
        "\r\n",
        "con_mat = tf.math.confusion_matrix(labels=test_labels, predictions=predictions).numpy()\r\n",
        "\r\n",
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\r\n",
        "\r\n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\r\n",
        "                     index = CLASSES,\r\n",
        "                     columns = CLASSES)\r\n",
        "\r\n",
        "figure = plt.figure(figsize=(8, 8))\r\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\r\n",
        "plt.tight_layout()\r\n",
        "plt.ylabel('True label')\r\n",
        "plt.xlabel('Predicted label')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}